# Sound2Hap


Under Construction...

For running model inferences, please go to Encodec https://github.com/facebookresearch/encodec and clone it under your inferencing path


The four signal processing algorithms for audio-to-vibration are adapted from below:  

- Frequench Shifting: @incollection{okazaki2015effect, title={The effect of frequency shifting on audio--tactile conversion for enriching musical experience}, author={Okazaki, Ryuta and Kuribayashi, Hidenori and Kajimoto, Hiroyuki}, booktitle={Haptic Interaction: Perception, Devices and Applications}, pages={45--51}, year={2015}, publisher={Springer} }  

- Perceptual Mapping: @inproceedings{lee2013real, title={Real-time perception-level translation from audio signals to vibrotactile effects}, author={Lee, Jaebong and Choi, Seungmoon}, booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems}, pages={2567--2576}, year={2013} }  

- Pitch Match: @article{kim2023sound, title={Sound-to-touch crossmodal pitch matching for short sounds}, author={Kim, Dong-Geun and Lee, Jungeun and Yun, Gyeore and Tan, Hong Z and Choi, Seungmoon}, journal={IEEE Transactions on Haptics}, volume={17}, number={1}, pages={2--7}, year={2023}, publisher={IEEE} }  
The MATLAB implementation is used for actual study  
The python version is used for Sound2Hap Web Tool's generation  

- HapticGen: @inproceedings{sung2025hapticgen, title={HapticGen: Generative Text-to-Vibration Model for Streamlining Haptic Design}, author={Sung, Youjin and John, Kevin and Yoon, Sang Ho and Seifi, Hasti}, booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems}, pages={1--24}, year={2025}  



